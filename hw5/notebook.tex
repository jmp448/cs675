
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{FATE}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Debiasing Word Embeddings}\label{debiasing-word-embeddings}

\subsubsection{What are Word
Embeddings?}\label{what-are-word-embeddings}

Word Embeddings are the result of applying dimensionality reduction
techniques to words!

They give us dense representations of words, which we hope capture some
syntactic and semantic information of the words. These dense
representations are a natural tool for us to use if we want to pass
words into neural models, and the field of Natural Language Processing
(NLP) has used some variants of Word Embeddings extensively.

There are many, many, many ways to build word embeddings, but the key
intuition comes from the notion that the meaning of a word can by
inferred from the types of words it appears next to, or as put by John
Firth in 1975: "You shall know a word by the company it keeps".

Typically, we start with a large corpora of text. We'll use this copora
to give us counts of words which occur next to each other, which is a
pretty good start. In this matrix, our rows correspond to an "embedding"
of sorts, where each word's embedding is a word by word count of the
words that appear next to it.

It's a hyperparameter, to choose how large of a window you use when
considering words "next to each other", or if you want to do clever
things, like weight each word by how distant it is from the word you're
looking at.

Here's what a word co-occurence matrix looks like:
{source:http://web.stanford.edu/class/cs224u/materials/cs224u-vsm-overview.pdf}

Each row now captures \emph{something} about the word it represents, in
that words that appear in similar contexts will be closer together. Of
course, they might not actually be \emph{that} close together, since our
space is the size of our vocabulary.

How big is our Vocabulary? Well, that's another hyperparameter. You can
decide to filter out words that don't occur that often, to potentially
get rid of noise, etc. But generally, it's going to be close to 300K or
400K. That means our word vectors are of dimension 400,000! Luckily, we
can use dimensionality reduction techniques, like the ones you've seen
already, to learn a low dimensional representation of this co-occurrence
matrix. This will give us low-dimensional (200-300d), dense word vectors
to use, so we can operate over them efficiently and pass them into
models such as neural networks!

Which dimensionality reduction technique should we use? Should we
normalize counts? Convert them to probability distributions and minimize
things like KL divergence? These are all design choices which can have a
big effect on the quality and output of your word vectors.

For this assignment, though, we'll be using a very standard set of Word
Embeddings, called GloVe (Global Vectors for Word Representations
https://nlp.stanford.edu/projects/glove/) These word embeddings were
standard in state of the art english NLP models, until very recently.

Let's load them up and take a look at them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k}{import} \PY{n}{norm}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{def} \PY{n+nf}{load\PYZus{}vecs}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Loads in word vectors from path.}
        \PY{l+s+sd}{    Will return a dictionary of word to index, and a matrix of vectors (each word is a row)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{vecs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{w2i} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
            
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{inp}\PY{p}{:}
                \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{inp}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                    \PY{n}{line} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
                    \PY{n}{word} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{line}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                    \PY{n}{w2i}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vecs}\PY{p}{)}
                    \PY{n}{vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{line}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{float}\PY{p}{)}\PY{p}{)}
                \PY{n}{vecs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{v} \PY{o}{/} \PY{n}{norm}\PY{p}{(}\PY{n}{v}\PY{p}{)} \PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n}{vecs}\PY{p}{]}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Read in }\PY{l+s+si}{\PYZob{}vecs.shape[0]\PYZcb{}}\PY{l+s+s1}{ words of size }\PY{l+s+si}{\PYZob{}vecs.shape[1]\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{w2i}\PY{p}{,} \PY{n}{vecs}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} This might take a little bit to run!}
        \PY{n}{indxr}\PY{p}{,} \PY{n}{wembs} \PY{o}{=} \PY{n}{load\PYZus{}vecs}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/glove.6B.100d.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Read in 400000 words of size 100

    \end{Verbatim}

    These word vectors capture some interesting semantic information!

Somewhat importantly, there seems to be some notion of semantics
captured in the \emph{vector difference} between two words. This led to
the very popular "analogy" game, where it was found you could take the
difference of two vectors, add it to another vector, and get this sort
of analogy of the first two, compared to the second.

The canonical example here is "Man is to King as Woman is to"... And it
turns out, when you use GloVe word embeddings to do this task, you get
"Queen"! Which is pretty cool!

Here are some examples of the embedding space, and you can kind of see
why this works!

The distance between man and woman and king and queen looks similar, but
so does the direction! So it makes sense that the vector difference
between woman and man, added to king, looks like queen.

Let's do this ourselves, and take a loot at it.

We've given you the similarity function \textasciitilde{}

\textbf{Implement the \texttt{analogy} method in the cell below.} You
are free to write helper functions as you wish, as well. The
\texttt{analogy} function should take in 4 arguments:
\texttt{n,\ word1,\ word2,\ word3} which reflects the following analogy:
"word1 is to word3 as word2 is to... result".

Your function should return the top \texttt{n} vectors, in order of
\textbf{most similar to least, as measured by cosine distance} which
reflect this analogy and \textbf{are not word1, word2, or word3}.

Remember that the analogy intuition comes from the fact that
\(w1 - w2 \approx w3 - w4\), so your function should be searching for
vectors which are similar to \(w4 \approx w2 - w1 + w3\)!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{similarity}\PY{p}{(}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{bump}\PY{p}{(}\PY{n}{l}\PY{p}{,} \PY{n}{newb}\PY{p}{,} \PY{n}{pos}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{l}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{n}\PY{p}{:}
                \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{l}\PY{p}{[}\PY{p}{:}\PY{n}{pos}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{newb}\PY{p}{]}\PY{p}{,} \PY{n}{l}\PY{p}{[}\PY{n}{pos}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{l}\PY{p}{[}\PY{p}{:}\PY{n}{pos}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{newb}\PY{p}{]}\PY{p}{,} \PY{n}{l}\PY{p}{[}\PY{n}{pos}\PY{p}{:}\PY{n}{n} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        
        \PY{k}{def} \PY{n+nf}{find\PYZus{}neighbors}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{neighborhood}\PY{p}{,} \PY{n}{new}\PY{p}{)}\PY{p}{:}
            \PY{n}{sims} \PY{o}{=} \PY{p}{[}\PY{n}{similarity}\PY{p}{(}\PY{n}{new}\PY{p}{,} \PY{n}{neighborhood}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}
            \PY{n}{nearest} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{int}\PY{p}{)}
            \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{neighborhood}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sims}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{this\PYZus{}sim} \PY{o}{=} \PY{n}{similarity}\PY{p}{(}\PY{n}{neighborhood}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{,} \PY{n}{new}\PY{p}{)}
                    \PY{k}{if} \PY{n}{this\PYZus{}sim} \PY{o}{\PYZgt{}} \PY{n}{sims}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                        \PY{n}{sims} \PY{o}{=} \PY{n}{bump}\PY{p}{(}\PY{n}{sims}\PY{p}{,} \PY{n}{this\PYZus{}sim}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{k}\PY{p}{)}
                        \PY{n}{nearest} \PY{o}{=} \PY{n}{bump}\PY{p}{(}\PY{n}{nearest}\PY{p}{,} \PY{n}{n}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{k}\PY{p}{)}
                        \PY{k}{break}
            \PY{k}{return} \PY{n}{nearest}
        
        
        \PY{k}{def} \PY{n+nf}{find\PYZus{}words}\PY{p}{(}\PY{n}{neighbors}\PY{p}{)}\PY{p}{:}
            \PY{n}{words} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{ind} \PY{o+ow}{in} \PY{n}{neighbors}\PY{p}{:}
                \PY{n}{words}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{indxr}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
            \PY{k}{return} \PY{n}{words}
        
        
        \PY{k}{def} \PY{n+nf}{analogy}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{word1}\PY{p}{,} \PY{n}{word2}\PY{p}{,} \PY{n}{word3}\PY{p}{)}\PY{p}{:}
            \PY{n}{emb1} \PY{o}{=} \PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word1}\PY{p}{]}\PY{p}{]}
            \PY{n}{emb2} \PY{o}{=} \PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word2}\PY{p}{]}\PY{p}{]}
            \PY{n}{emb3} \PY{o}{=} \PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word3}\PY{p}{]}\PY{p}{]}
            \PY{n}{emb4} \PY{o}{=} \PY{n}{emb2} \PY{o}{\PYZhy{}} \PY{n}{emb1} \PY{o}{+} \PY{n}{emb3}
            \PY{n}{neighbors} \PY{o}{=} \PY{n}{find\PYZus{}neighbors}\PY{p}{(}\PY{n}{n}\PY{o}{+}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{wembs}\PY{p}{,} \PY{n}{emb4}\PY{p}{)}
            \PY{n}{top\PYZus{}words} \PY{o}{=} \PY{n}{find\PYZus{}words}\PY{p}{(}\PY{n}{neighbors}\PY{p}{)}
            \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{p}{[}\PY{n}{word1}\PY{p}{,} \PY{n}{word2}\PY{p}{,} \PY{n}{word3}\PY{p}{]}\PY{p}{:}  \PY{c+c1}{\PYZsh{} filter out any repeated words \PYZhy{} boring analogies}
                \PY{k}{if} \PY{n}{top\PYZus{}words}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}contains\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{:}
                    \PY{n}{top\PYZus{}words}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{w}\PY{p}{)}
            \PY{k}{return} \PY{n}{top\PYZus{}words}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}
\end{Verbatim}


    Once you've implemented your function, you can run the cell below to
test it.

For reference, the top result should be queen!!

Do not change the below cell when turning in the notebook. You should
run this cell as is, and leave the output when you have the function
correct!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{king}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['queen', 'monarch', 'throne', 'daughter', 'princess', 'prince', 'elizabeth', 'mother', 'emperor', 'wife']

    \end{Verbatim}

    We are going to use the cell above to help us evaluate your function.

However, you should be curious about this "analogy" function! Play
around with it in the cell below. Try different words! (Remember that we
have a limited vocabulary... You don't need to handle OOV words nicely,
but know that your code can crash occasionally if you pass in a word
that's not in our vocabulary!)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{strong}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{obama}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{merkel}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{america}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['stronger', 'weak', 'robust', 'strongest', 'despite', 'support', 'growing', 'concern', 'particularly', 'reflected']
['europe', 'germany', 'european', 'france', 'scandinavia', 'german', 'eu', 'world', 'continent', 'baltic']

    \end{Verbatim}

    In this cell, tell us about the most interesting analogy you've found!

It can be any analogy you've discovered, whether you think it's
interesting, wrong, or even problematic! Give a short description about
why you chose this analogy. You should limit your analogies to be chosen
from the top 10 results of your analogy function (that is, with
\texttt{n=10}).

\textbf{Analogy}: "Man is to strong as woman is to \emph{stronger}" (top
result)

\textbf{Discussion}: The future is female.

\textbf{Analogy}: "Obama is to America as Merkel is to \emph{Europe}"
(top result, \emph{Germany} second result)

\textbf{Discussion}: Interesting that it's pretty successful in this
analogy involving proper nouns. As a follow up check, to see what kind
of resolution this line of analogies would have, I also substituted
napoleon in for obama, and rather than giving the same result it
broadened to "global", and then some broad terms involving politics/
economics, not picking up on Germany at all (which it shouldn't
considering Napoleon's connection to America is not similar to Merkel's
connection to Germany).

    \subsubsection{(Gender) Bias in Word
Embeddings}\label{gender-bias-in-word-embeddings}

Word embeddings are a very useful tool in NLP, and they have often
helped researchers boost their performance in a variety of tasks.
However, recently a large problem has been discovered in these types of
word embeddings. They inherit some biases from the data they are trained
on which is very harmful, such as mysoginistic or racist stereotypes.
This is a \textbf{huge} problem, because models which use these
embeddings are being deployed into the real word to assist with
automation strategies!

Let's take a look at some examples of gender bias in our word
embeddings, which we'll be focusing on for the rest of this assignment.

Do not change the below cell! Make sure you run it as is before turning
in your notebook.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{programmer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Even names contain these biases!}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{john}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['educator', 'programmers', 'linguist', 'technician', 'freelance', 'animator', 'translator', 'software', 'psychotherapist', 'technologist']
['nurse', 'physician', 'doctors', 'patient', 'dentist', 'pregnant', 'medical', 'nursing', 'mother', 'hospital']
['nurse', 'mother', 'nursing', 'woman', 'dentist', 'pregnant', 'hospital', 'patient', 'girl', 'grandmother']

    \end{Verbatim}

    What does it look like if we swap the analogy around?

Do not change the below cell! Make sure you run it as is before turning
in your notebook.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{programmer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{john}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['programmers', 'software', 'computer', 'animator', 'engineer', 'setup', 'mechanic', 'compiler', 'animators', 'developer']
['dr.', 'brother', 'him', 'he', 'himself', 'physician', 'father', 'master', 'friend', 'taken']
['physician', 'he', 'dr.', 'surgeon', 'himself', 'him', 'asked', 'man', 'expert', 'agent']

    \end{Verbatim}

    This is obviously problematic. Yet, these word embeddings (the ones
you're using right now) \textbf{have been used in multiple state of the
art systems in NLP!} This is a hot area of research right now, because
this poses a huge potential problem as more and more AI systems are
starting to be deployed into the real world.

We're going to take a look at one method of \emph{debiasing} these word
embeddings, which attempts to \emph{remove gender stereotypes} while
keeping in useful gender information such as "king and queen" and "boy
and girl". The debiasing method we're going to look at is described in
\href{https://arxiv.org/abs/1607.06520}{this paper}, which is one of the
seminal papers on exposing stereotypes and bias in this form.

While they use different word embeddings, we've seen that our GloVe
embeddings contain similar biases. Their method behaves as follows.

They first define a "gender subspace"
\(\mathcal{B} = (b_1, b_2, ..., b_k)\) composed of \emph{orthogonal
vectors}. \(k\) is a hyperparameter we choose.

\(\mathcal{B}\) is built from a set of pairs of gendered items. The idea
here is that \(\mathcal{B}\) captures some notion of the "direction" of
gender which we're trying to capture.

The set of pairs, \(S = p_1, p_2, ..., p_n\), is given to you. Each pair
contains two words which are considered gendered words whose relation
captures some notion of gender, i.e. \$S = \$\{("woman", "man"), ("she",
"he")...\}

Building \(\mathcal{B}\) goes as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build up matrix
  \(\mathbf{C} := \sum_{i=1}^n (\vec{w_1} - \vec{w_2})(\vec{w_1} - \vec{w_2})^T + (\vec{w_2} - \vec{w_1})(\vec{w_2} - \vec{w_1})^T\),
  for \((w_1, w_2) \in p_i\).
\end{enumerate}

That is, for each pair, subtract each word vector from the other and
take the outer product of each resulting vector and add it to
\(\mathbf{C}\). In our case, \(\mathbf{C}\)'s dimensionality should be
100 by 100.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute the SVD of \(\mathbf{C}\).
\end{enumerate}

You can use numpy's \texttt{numpy.linalg.svd} method for this.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  This will give you a decomposition
  \(\mathbf{U}\mathbf{\Sigma}\mathbf{V} = \mathbf{C}\). Take the
  top-\(k\) vectors from the decomposition of \(\mathbf{C}\) as the
  orthogonal vectors defining the space
  \(\mathcal{B} = (b_1, ..., b_k)\). That is, you should take the first
  \(k\) columns of \(\mathbf{U}\).
\end{enumerate}

Again, the intuition here is that we now have some set of vectors which,
together, capture some notion of the direction of gender.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Copy biased embeddings into a new object.}
        \PY{n}{debiased\PYZus{}wembs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{wembs}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k}{import} \PY{n}{svd}
         \PY{n}{gender\PYZus{}pairs} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{she}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{he}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{her}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{his}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{john}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{herself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{himself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daughter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{son}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mother}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{father}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{guy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{girl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{female}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{male}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
         
         \PY{k}{def} \PY{n+nf}{build\PYZus{}gender\PYZus{}subspace}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
             \PY{n}{C} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}\PY{p}{)}
             \PY{k}{for} \PY{n}{pair} \PY{o+ow}{in} \PY{n}{gender\PYZus{}pairs}\PY{p}{:}
                 \PY{n}{emb1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{pair}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}
                 \PY{n}{emb2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{pair}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}
                 \PY{n}{onetwo} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{emb1}\PY{o}{\PYZhy{}}\PY{n}{emb2}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{emb1}\PY{o}{\PYZhy{}}\PY{n}{emb2}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{C}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                 \PY{n}{twoone} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{emb2}\PY{o}{\PYZhy{}}\PY{n}{emb1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{emb2}\PY{o}{\PYZhy{}}\PY{n}{emb1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{C}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                 \PY{n}{C} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{C}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{emb1}\PY{o}{\PYZhy{}}\PY{n}{emb2}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{emb1}\PY{o}{\PYZhy{}}\PY{n}{emb2}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{emb2}\PY{o}{\PYZhy{}}\PY{n}{emb1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{emb2}\PY{o}{\PYZhy{}}\PY{n}{emb1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{]} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{C}\PY{p}{)}
             \PY{k}{return} \PY{n}{u}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n}{k}\PY{p}{]}
\end{Verbatim}


    Now let's build the subspace with \(k=10\). You can check that things
seem ok by making sure that the dot product between all your \(b_i\)
vectors is close to zero, since they should be orthogonal.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{B} \PY{o}{=} \PY{n}{build\PYZus{}gender\PYZus{}subspace}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    We'll only implement the neutralize" portion of the hard-debiasing
method in the paper, if you're following along. We won't implement
equalize or the Soft-Debiasing method, to keep things short :)

Once we have our gender subspace \(\mathcal{B}\) composed of our \(k\)
orthogonal vectors, we can select some choice word \(w\) to debias as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Select the embedding \(\vec{w}\) of word \(w\) from our regular,
  biased embeddings.
\item
  Compute
  \[\vec{w_\mathcal{B}} = \sum_{j=i}^k (\vec{w} \cdot \vec{b_j}) * \vec{b_j}\].
\item
  Compute the new, debiased embedding as
  \[\vec{w_{ub}} = (\vec{w} - \vec{w_{\mathcal{B}}}) \; / \; || \vec{w} - \vec{w_{\mathcal{B}}} || \]
\end{enumerate}

Intuitively, what we are doing is projecting our biased vector
\(\vec{w}\) into our gender subspace, and then subtracting the result
from \(\vec{w}\).

You should implement a function \texttt{debias\_word(word)}, which takes
one argument: the word to debias. It should use our previously defined
subspace \texttt{B} to compute \(\vec{w_{ub}}\), and you should store
the result in the new \texttt{debiased\_wembs} matrix defined above,
\textbf{in the same index that the word is in the original \texttt{wemb}
matrix}.

That is, please do not change the \texttt{wembs} matrix directly, but
save your debiased embeddings in the copy we created.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{debias\PYZus{}word}\PY{p}{(}\PY{n}{word}\PY{p}{)}\PY{p}{:}
             \PY{n}{emb} \PY{o}{=} \PY{n}{wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{]}
             \PY{n}{wb} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{emb}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{B}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{wb} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{wb}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{emb}\PY{p}{,} \PY{n}{B}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{n}{B}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{p}{)}
             \PY{n}{wub} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{emb}\PY{p}{,} \PY{n}{wb}\PY{p}{)}\PY{o}{/}\PY{n}{norm}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{emb}\PY{p}{,} \PY{n}{wb}\PY{p}{)}\PY{p}{)}
             \PY{n}{debiased\PYZus{}wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{wub}
             \PY{k}{return} \PY{n}{debiased\PYZus{}wembs}
\end{Verbatim}


    Lastly, let's build a \emph{new} analogy function, called
\texttt{debiased\_analogy} which operates exactly the same the your
original analogy function, but uses the \texttt{debiased\_wembs}
instead.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{debiased\PYZus{}analogy}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{word1}\PY{p}{,} \PY{n}{word2}\PY{p}{,} \PY{n}{word3}\PY{p}{)}\PY{p}{:}
             \PY{n}{emb1} \PY{o}{=} \PY{n}{debiased\PYZus{}wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word1}\PY{p}{]}\PY{p}{]}
             \PY{n}{emb2} \PY{o}{=} \PY{n}{debiased\PYZus{}wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word2}\PY{p}{]}\PY{p}{]}
             \PY{n}{emb3} \PY{o}{=} \PY{n}{debiased\PYZus{}wembs}\PY{p}{[}\PY{n}{indxr}\PY{p}{[}\PY{n}{word3}\PY{p}{]}\PY{p}{]}
             \PY{n}{emb4} \PY{o}{=} \PY{n}{emb2} \PY{o}{\PYZhy{}} \PY{n}{emb1} \PY{o}{+} \PY{n}{emb3}
             \PY{n}{neighbors} \PY{o}{=} \PY{n}{find\PYZus{}neighbors}\PY{p}{(}\PY{n}{n}\PY{o}{+}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{debiased\PYZus{}wembs}\PY{p}{,} \PY{n}{emb4}\PY{p}{)}
             \PY{n}{top\PYZus{}words} \PY{o}{=} \PY{n}{find\PYZus{}words}\PY{p}{(}\PY{n}{neighbors}\PY{p}{)}
             \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{p}{[}\PY{n}{word1}\PY{p}{,} \PY{n}{word2}\PY{p}{,} \PY{n}{word3}\PY{p}{]}\PY{p}{:}  \PY{c+c1}{\PYZsh{} filter out any repeated words \PYZhy{} boring analogies}
                 \PY{k}{if} \PY{n}{top\PYZus{}words}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}contains\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{:}
                     \PY{n}{top\PYZus{}words}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{w}\PY{p}{)}
             \PY{k}{return} \PY{n}{top\PYZus{}words}
\end{Verbatim}


    Now we're ready to start debiasing our word vectors!

We've picked out a few choice words to debiase for the purpose of our
example. Let's debias them, and then revisit our "man is to doctor as
woman is to" analogy.

Do not change the below cell! Make sure you run it as is before turning
in your notebook. It will be used for grading.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{nurse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dentist}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{patient}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{physician}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dr.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{boss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{debiased\PYZus{}analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{debiased\PYZus{}analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doctor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['physician', 'nurse', 'patient', 'dr.', 'doctors', 'medical', 'pregnant', 'pregnancy', 'pediatrician', 'nursing', 'psychiatrist', 'pharmacist']
['physician', 'nurse', 'patient', 'dr.', 'doctors', 'his', 'dentist', 'brother', 'he', 'described', 'admitted']

    \end{Verbatim}

    This is looking better! Not only is the top result a lot better, but
there seems to be much less of a difference between the swapped results
as well. Very nice!

Our solution doesn't scale, obviously. We can't expect ourselves to
manually type each word that we think should be debiased. However, this
is a decent start!

There is a lot of research in this area right now, and people are
constantly coming up with better and more scalable methods to solve this
very real problem!

To close things off, provide us with one more example of an analogy you
found that seemed gender-biased.

Debias the words that you think are necessary to debias the analogy, and
then report the new analogy in the cells below.

Biased Analogy you found: "Mary is to schoolteacher as John is to
\emph{businessman}/ \emph{bricklayer}/ \emph{politician}", while "John
is to schoolteacher as Mary is to \emph{homemaker}/ \emph{housewife}/
\emph{widowed}"

Debias the words below, and then print out the new debiased analogy and
it's gender-swap!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{debias\PYZus{}word}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{schoolteacher}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{debiased\PYZus{}analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{john}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{schoolteacher}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{debiased\PYZus{}analogy}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{john}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{schoolteacher}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['businessman', 'bricklayer', 'loner', 'electrician', 'mechanic', 'stockbroker', 'banker', 'politician', 'veteran', 'rancher', 'accountant', 'congressman']
['homemaker', 'housewife', 'seamstress', 'tomboy', 'stepdaughter', 'spinster', 'pensioner', 'widowed', 'grandmother', 'midwife', 'housekeeper', 'mother']

    \end{Verbatim}

    Unfortunately, looks like this one was not successfully debiased using
the same means as were used to debias "doctor". Perhaps there is more
bias on this one - I personally do know many more female doctors than I
do male schoolteachers. It also may not help that "schoolteacher" is a
slightly outdated term. I'd say that gender bias in writing has gotten a
bit better with time, but if a term stopped being used before this
change arose then it's safe to expect it to reflect greater bias.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
